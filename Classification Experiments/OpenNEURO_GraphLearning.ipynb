{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Version: 2.14.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "from numpy.linalg import matrix_power as MP\n",
    "\n",
    "import random\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "import networkx as nx\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.math import scalar_mul as c_mul\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "print(f\"TF Version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 152 166\n"
     ]
    }
   ],
   "source": [
    "OpenNEURO_dataset = loadmat(\"./Dataset/OpenNERO_32_and_132_data.mat\")\n",
    "OpenNEURO_dataset_132 = OpenNEURO_dataset['OpenNERO_132_roidata']\n",
    "OpenNEURO_dataset_32 = OpenNEURO_dataset['OpenNERO_32_roidata']\n",
    "OpenNEURO_labels = [0] * 117 + [1] * 49 # 0 - Control, 1 - Patient\n",
    "\n",
    "X_train_data = OpenNEURO_dataset_32\n",
    "y_train_data = OpenNEURO_labels\n",
    "\n",
    "N, T, num_data = X_train_data.shape\n",
    "print(N, T, num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_delta(T):\n",
    "    diff_matrix = np.zeros([T, T - 1]) - np.eye(T, T - 1)\n",
    "    for i in range(1, T):\n",
    "        diff_matrix[i, i - 1] = 1\n",
    "    return tf.convert_to_tensor(diff_matrix, dtype = \"float32\")\n",
    "\n",
    "def adj_to_laplacian(AdjL):\n",
    "    tmp = np.array(AdjL, dtype = \"float32\")\n",
    "    return np.diag(np.sum(tmp, axis = 0)) - tmp\n",
    "\n",
    "def initial_graph(X_training, N, top_values = 6):\n",
    "    distance_matrix = cdist(X_training, X_training, metric = 'euclidean')\n",
    "    \n",
    "    # top_values = 6\n",
    "    sorted_indices = distance_matrix.argsort(1)\n",
    "    TAdj = np.zeros([N, N])\n",
    "    \n",
    "    for i in range(N):\n",
    "        for j in range(1, top_values):\n",
    "            k = sorted_indices[i][j]\n",
    "            TAdj[i][k] = 1\n",
    "    \n",
    "    TAdj = (TAdj + TAdj.T) / 2\n",
    "    TAdj = np.where(TAdj > 0.1, 1, 0)\n",
    "    \n",
    "    Laplacian_init = np.diag(np.sum(TAdj, 1)) - TAdj\n",
    "    \n",
    "    return Laplacian_init\n",
    "\n",
    "def label_print(idx, length):\n",
    "    l = len(str(idx))\n",
    "    if l < length:\n",
    "        return \"0\" * (length - l) + str(idx)\n",
    "    else:\n",
    "        return str(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.  0.  0. ...  0.  0.  0.]\n",
      " [ 1. -1.  0. ...  0.  0.  0.]\n",
      " [ 0.  1. -1. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0. ...  1. -1.  0.]\n",
      " [ 0.  0.  0. ...  0.  1. -1.]\n",
      " [ 0.  0.  0. ...  0.  0.  1.]], shape=(152, 151), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "d = generate_delta(T)\n",
    "print(d)\n",
    "\n",
    "Mul = np.eye(N) - np.ones([N, N])\n",
    "Mul = tf.convert_to_tensor(Mul, dtype = \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = 0\n",
    "psi_cost = 0\n",
    "Laplacian = 0\n",
    "A_transform_cost = 0\n",
    "\n",
    "model_pred = []\n",
    "loss_model = []\n",
    "learned_graph = []\n",
    "mse_known_list = []\n",
    "mse_unknown_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 4)\n"
     ]
    }
   ],
   "source": [
    "coeffs_list = np.load(\"./Outputs/OpenNEURO_cost_func_coeffs.npy\")\n",
    "print(coeffs_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A - Target, B - Predicted\n",
    "def costfunc(A, B):\n",
    "    global coeffs\n",
    "    global X_train\n",
    "    global psi_cost\n",
    "    global Laplacian\n",
    "    global A_transform_cost\n",
    "\n",
    "    B_tmp = tf.squeeze(B)\n",
    "    data_fidelity = (tf.norm(tf.multiply(psi_cost, (X_train - B))) ** 2) * coeffs[0]\n",
    "    smoothness = tf.linalg.trace(tf.transpose(B_tmp) @ Laplacian @ B_tmp @ A_transform_cost) * coeffs[1]\n",
    "    L_cost = (tf.norm(Laplacian) ** 2) * coeffs[2]\n",
    "    Z_cost = (tf.norm(A_transform_cost) ** 2) * coeffs[3]\n",
    "    \n",
    "    tmp = (smoothness) + (data_fidelity) + (L_cost) + (Z_cost)\n",
    "    \n",
    "    global loss_model\n",
    "    loss_model.append(tmp)\n",
    "    \n",
    "    return tmp\n",
    "    \n",
    "def costfunc1(A, B):\n",
    "    global X_train\n",
    "    global psi_cost\n",
    "    \n",
    "    data_fidelity = (tf.norm(tf.multiply(psi_cost, (X_train - B))) ** 2)\n",
    "    return (data_fidelity)\n",
    "\n",
    "def costfunc2(A, B):\n",
    "    global Laplacian\n",
    "    global A_transform_cost\n",
    "    \n",
    "    B_tmp = tf.squeeze(B)\n",
    "    smoothness = tf.linalg.trace(tf.transpose(B_tmp) @ Laplacian @ B_tmp @ A_transform_cost)\n",
    "    \n",
    "    return (smoothness)\n",
    "\n",
    "def costfunc3(A, B):\n",
    "    global Laplacian\n",
    "    \n",
    "    L_cost = (tf.norm(Laplacian) ** 2)\n",
    "    \n",
    "    return (L_cost)\n",
    "\n",
    "def costfunc4(A, B):\n",
    "    global A_transform_cost\n",
    "    \n",
    "    Z_cost = (tf.norm(A_transform_cost) ** 2)\n",
    "    \n",
    "    return (Z_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_unknown(A, B):\n",
    "    \n",
    "    global psi_cost\n",
    "    psi = psi_cost\n",
    "    \n",
    "    psid = tf.convert_to_tensor(np.ones([N, T], dtype = float) - psi, dtype = \"float32\")\n",
    "    mse_unk = (LA.norm(np.multiply(psid, (X_train - B))) ** 2) / np.sum(psid.numpy().flatten())\n",
    "    \n",
    "    global mse_unknown_list\n",
    "    mse_unknown_list.append(mse_unk)\n",
    "    \n",
    "    return mse_unk\n",
    "\n",
    "def mse_known(A, B):\n",
    "    \n",
    "    global psi_cost\n",
    "    psi = psi_cost\n",
    "    \n",
    "    mse_kn = (LA.norm(np.multiply(psi, (X_train - B))) ** 2) / np.sum(psi.numpy().flatten())\n",
    "    \n",
    "    global model_pred\n",
    "    model_pred.append([A.numpy(), B.numpy()])\n",
    "    \n",
    "    global mse_known_list\n",
    "    mse_known_list.append(mse_kn)\n",
    "    \n",
    "    return mse_kn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callb = EarlyStopping(monitor = 'loss', mode = 'min', min_delta = 0.0001, verbose = 1, patience = 10)\n",
    "\n",
    "lr_decay_factor = 1.2\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 3:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr / lr_decay_factor\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposedModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, L_init, reg_init, loops_init, d_init, p_init, alpha_init, beta_init, threshold_in, **kwargs):\n",
    "        super(ProposedModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.p1_init = tf.constant(p_init[0], shape = (1, 1), dtype = \"float32\")\n",
    "        self.p1 = tf.Variable(initial_value = self.p1_init, trainable = True, name = \"p1\")\n",
    "        \n",
    "        self.p2_init = tf.constant(p_init[1], shape = (1, 1), dtype = \"float32\")\n",
    "        self.p2 = tf.Variable(initial_value = self.p2_init, trainable = True, name = \"p2\")\n",
    "        \n",
    "        self.L_in = L_init\n",
    "        self.preg = reg_init\n",
    "        self.loops = loops_init\n",
    "        self.d_int = d_init\n",
    "        self.alp = alpha_init\n",
    "        self.bet = beta_init\n",
    "        self.threshold_init = threshold_in\n",
    "        \n",
    "    def data_inpainting(self, psi, Y, Lk, AAT):\n",
    "        loop_b, reg = self.loops[1], self.preg\n",
    "\n",
    "        Xk = tf.zeros_like(Y)\n",
    "        Zk = -(tf.multiply(psi, Xk) - Y + (2 * reg * (Lk @ Xk @ AAT)))\n",
    "        \n",
    "        for i in range(loop_b):\n",
    "            fdx_xk = tf.multiply(psi, Xk) - Y + (2 * reg * (Lk @ Xk @ AAT))\n",
    "\n",
    "            # row_print = 4\n",
    "            # tmp = list(zip((tf.multiply(psi, Xk) - Y)[:, row_print].numpy(), (2 * reg * (Lk @ Xk @ AAT)[:, row_print].numpy())))\n",
    "            # print(\"\\n\\nStart here\")\n",
    "            # for k in tmp:\n",
    "            #     print(k)\n",
    "            \n",
    "            fdx_zk = tf.multiply(psi, Zk) - Y + (2 * reg * (Lk @ Zk @ AAT))\n",
    "            \n",
    "            tau = tf.linalg.trace(tf.transpose(fdx_xk) @ Zk) / tf.linalg.trace(tf.transpose((Y + fdx_zk)) @ Zk)\n",
    "            \n",
    "            Xk_1 = Xk - (tau * Zk)\n",
    "\n",
    "            fdx_xk_1 = tf.multiply(psi, Xk_1) - Y + (2 * reg * (Lk @ Xk_1 @ AAT))\n",
    "            gamma = (tf.norm(fdx_xk_1) ** 2) / (tf.norm(fdx_xk) ** 2)\n",
    "\n",
    "            Zk_1 = (gamma * Zk) - fdx_xk_1\n",
    "\n",
    "            Xk = Xk_1\n",
    "            Zk = Zk_1\n",
    "        # print(\"\\n\\nEnds here\\n\\n\")\n",
    "        return Xk_1 \n",
    "    \n",
    "    def graph_learning(self, X, Lk, AAT):\n",
    "        loop_c, alpha, beta = self.loops[2], self.alp, self.bet\n",
    "        \n",
    "        # Computing the smoothness term\n",
    "        X_AAT_XT = tf.matmul(X, tf.matmul(AAT, tf.transpose(X)))\n",
    "        \n",
    "        for i in range(loop_c):\n",
    "\n",
    "            # Extracting the adjacency matrix from the Laplacian\n",
    "            adj_i = tf.multiply(Mul, Lk)\n",
    "\n",
    "            # Computing the gradient with respect to laplacian\n",
    "            fdL_Lk = X_AAT_XT + (beta * Lk)\n",
    "            \n",
    "            # row_print = 4\n",
    "            # print(\"For beta\")\n",
    "            # tmp = list(zip(X_AAT_XT[row_print, :].numpy(), np.array(beta * Lk)[row_print, :]))\n",
    "            # for k in tmp:\n",
    "            #     print(k)\n",
    "            \n",
    "            # Extracting the adjacency matrix from the gradient\n",
    "            adj_grad = tf.multiply(Mul, fdL_Lk)\n",
    "            \n",
    "            # Computing the new adjacency\n",
    "            # tmp = list(zip(adj_i[row_print, :].numpy(), (alpha * adj_grad)[row_print, :].numpy()))\n",
    "            # if i == (loop_c-1):\n",
    "            #     print(\"\\nFor alpha\")\n",
    "            #     for k in tmp:\n",
    "            #         print(k)\n",
    "            #     print(\"\\nStart here\")\n",
    "            \n",
    "            adj_i1 = adj_i - (alpha * adj_grad)\n",
    "            \n",
    "            # Avoiding any negative values\n",
    "            adj_i1 = tf.nn.relu(adj_i1)\n",
    "\n",
    "            # Computing the new laplacian\n",
    "            Lk_1 = tf.linalg.diag(tf.math.reduce_sum(adj_i1, 1)) - adj_i1\n",
    "            \n",
    "            # Updating the laplacian\n",
    "            Lk = Lk_1\n",
    "        # print(\"\\n\\nEnds here\\n\\n\")\n",
    "        return Lk\n",
    "    \n",
    "\n",
    "    def call(self, y):\n",
    "\n",
    "        psi = y[0,:,T:]\n",
    "        global psi_cost\n",
    "        psi_cost = psi\n",
    "\n",
    "        psi = tf.convert_to_tensor(psi, dtype = \"float32\")\n",
    "        \n",
    "        X_in = y[0,:,:T]\n",
    "        X_in = psi * tf.convert_to_tensor(X_in, dtype = \"float32\")\n",
    "        Y = X_in\n",
    "        \n",
    "        d = self.d_int\n",
    "        LT = tf.matmul(d, tf.transpose(d))\n",
    "        LT = tf.convert_to_tensor(LT, dtype = \"float32\")\n",
    "        \n",
    "        A_transform = tf.eye(T) + (self.p1 * MP(LT, 1)) + (self.p2 * MP(LT, 2))\n",
    "        AAT = A_transform\n",
    "        \n",
    "        global A_transform_cost\n",
    "        A_transform_cost = AAT\n",
    "        \n",
    "        loop_a = self.loops[0]\n",
    "        threshold = self.threshold_init\n",
    "        Lk = self.L_in\n",
    "        \n",
    "        for i in range(loop_a):\n",
    "            \n",
    "            Xk_1 = self.data_inpainting(psi, Y, Lk, AAT)\n",
    "            \n",
    "            Xk_1_gt = X_in + tf.multiply((tf.ones(psi.shape) - psi), Xk_1)\n",
    "            \n",
    "            Lk_1 = self.graph_learning(Xk_1_gt, Lk, AAT)\n",
    "            \n",
    "            Adj = tf.linalg.diag(tf.linalg.diag_part(Lk_1)) - Lk_1 # Gives adjacency\n",
    "            \n",
    "            n = tf.math.count_nonzero(Adj, dtype=\"float32\")\n",
    "            mean = tf.math.reduce_sum(Adj) / n\n",
    "            Adj = tf.math.subtract(Adj, (threshold * mean)) # Ensures the sparsity of learned graph\n",
    "            \n",
    "            Adj = tf.nn.relu(Adj)\n",
    "            # Adj = tf.math.sign(Adj)\n",
    "            \n",
    "            Lk_1 = tf.linalg.diag(tf.math.reduce_sum(Adj, 1)) - Adj\n",
    "            \n",
    "            Lk = Lk_1\n",
    "        \n",
    "            global Laplacian\n",
    "            Laplacian = Lk\n",
    "        \n",
    "        Xk_1 = self.data_inpainting(psi, Y, Lk, AAT)\n",
    "        \n",
    "        global learned_graph\n",
    "        learned_graph.append(Lk.numpy())\n",
    "        \n",
    "        return tf.reshape(Xk_1, [1, N, T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_a = 5\n",
    "loop_b = 15 # Data Inpainting\n",
    "loop_c = 15 # Graph Learning\n",
    "\n",
    "# reg_init = 2.0e-7\n",
    "reg_init = 1.0e-7\n",
    "\n",
    "# alpha_init = 1.0e-4\n",
    "# beta_init = 1.0e1\n",
    "alpha_init = 5.0e-4\n",
    "beta_init = 1.0e1\n",
    "\n",
    "threshold_init = 0.2\n",
    "\n",
    "no_of_psi = 20\n",
    "sensing_ratio = 0.1\n",
    "\n",
    "p_init = [60.0, 120.0]\n",
    "lr_decay_factor = 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_vars = []\n",
    "# for dat in range(num_data):\n",
    "#     print(f\"Datapoint: {dat + 1}\")\n",
    "#     model_pred = []\n",
    "#     loss_model = []\n",
    "#     learned_graph = []\n",
    "\n",
    "#     X_train = X_train_data[:, :, dat]\n",
    "#     Lap_init = initial_graph(X_train, N, 6)\n",
    "    \n",
    "#     all_psi = []\n",
    "\n",
    "#     M = int(sensing_ratio * T)\n",
    "#     X_train_missing = np.zeros([no_of_psi, N, T])\n",
    "#     X_train_concatenated = np.zeros([no_of_psi, N, 2 * T])\n",
    "\n",
    "#     for i in range(no_of_psi):\n",
    "#         psi_k = np.array([0] * (N * M) + [1] * (N * (T - M)))\n",
    "#         np.random.shuffle(psi_k)\n",
    "#         psi_k = psi_k.reshape([N, T])\n",
    "#         all_psi.append(psi_k)\n",
    "#         X_train_missing[i, : , : ] = X_train * psi_k\n",
    "#         X_train_concatenated[i, :, :] = np.concatenate((X_train_missing[i], psi_k), axis = 1)\n",
    "    \n",
    "#     coeffs = coeffs_list[dat, :]\n",
    "\n",
    "#     sgd_optimizer = tf.keras.optimizers.SGD(learning_rate = 0.0001, momentum = 0.0)\n",
    "    \n",
    "#     our_model = ProposedModel(Lap_init, reg_init, [loop_a, loop_b, loop_c], \n",
    "#                               d, p_init, alpha_init, beta_init, threshold_init, name = 'our')\n",
    "#     our_model.compile(optimizer = sgd_optimizer, loss = costfunc,\n",
    "#                       metrics = [mse_known, mse_unknown], run_eagerly=True)\n",
    "    \n",
    "#     our_model.fit(X_train_concatenated, X_train_missing, epochs = 15, \n",
    "#                   callbacks = [lr_scheduler], batch_size = 1)\n",
    "    \n",
    "#     var = our_model.variables\n",
    "#     var_lst = []\n",
    "#     for v in var:\n",
    "#         var_lst.append(v.numpy()[0, 0])\n",
    "#     print(f\"\\nFilter Coefficients:\", var_lst)\n",
    "#     all_vars.append(var_lst)\n",
    "\n",
    "#     graph = learned_graph[-1]\n",
    "#     print(f\"\\nLearned Graph Shape:\\n{graph.shape}\")\n",
    "\n",
    "#     np.save(f\"./Outputs/Graphs/OpenNEURO/{label_print(dat, 3)}_{y_train_data[dat]}.npy\", graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"./Outputs/OpenNEURO_mse_known.npy\", mse_known_list)\n",
    "# np.save(\"./Outputs/OpenNEURO_mse_unknown.npy\", mse_unknown_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datapoint: 12\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 19s 953ms/step - loss: 3720360.5000 - mse_known: 0.6825 - mse_unknown: 1.7570 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 18s 904ms/step - loss: 3707797.5000 - mse_known: 0.6853 - mse_unknown: 1.7562 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 20s 1s/step - loss: 3695926.7500 - mse_known: 0.6877 - mse_unknown: 1.7558 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 18s 900ms/step - loss: 3685540.5000 - mse_known: 0.6898 - mse_unknown: 1.7553 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 12s 578ms/step - loss: 3677168.5000 - mse_known: 0.6916 - mse_unknown: 1.7550 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [56.510418, 126.00794]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 37\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 12s 582ms/step - loss: 1583636.0000 - mse_known: 0.1919 - mse_unknown: 0.7688 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 17s 850ms/step - loss: 1581566.7500 - mse_known: 0.1924 - mse_unknown: 0.7683 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 17s 883ms/step - loss: 1579506.3750 - mse_known: 0.1929 - mse_unknown: 0.7680 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 1577613.3750 - mse_known: 0.1934 - mse_unknown: 0.7677 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 17s 833ms/step - loss: 1576039.8750 - mse_known: 0.1938 - mse_unknown: 0.7674 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [59.004414, 122.72794]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 81\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 15s 767ms/step - loss: 831436.8750 - mse_known: 0.0694 - mse_unknown: 0.4808 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 17s 828ms/step - loss: 831280.8125 - mse_known: 0.0694 - mse_unknown: 0.4809 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 16s 815ms/step - loss: 831124.3125 - mse_known: 0.0694 - mse_unknown: 0.4810 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 16s 780ms/step - loss: 830979.6250 - mse_known: 0.0693 - mse_unknown: 0.4811 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 16s 799ms/step - loss: 830860.4375 - mse_known: 0.0693 - mse_unknown: 0.4812 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [59.20032, 119.95752]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 97\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 16s 784ms/step - loss: 821717.4375 - mse_known: 0.0665 - mse_unknown: 0.4476 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 27s 1s/step - loss: 821507.5000 - mse_known: 0.0664 - mse_unknown: 0.4478 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 28s 1s/step - loss: 821295.0000 - mse_known: 0.0664 - mse_unknown: 0.4479 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 821100.1875 - mse_known: 0.0663 - mse_unknown: 0.4480 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 26s 1s/step - loss: 820930.8125 - mse_known: 0.0663 - mse_unknown: 0.4481 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [59.06666, 119.97349]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 118\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 24s 1s/step - loss: 5096435.0000 - mse_known: 1.7462 - mse_unknown: 3.2485 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 30s 1s/step - loss: 4982652.0000 - mse_known: 1.7340 - mse_unknown: 3.2528 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 21s 1s/step - loss: 4889397.0000 - mse_known: 1.7223 - mse_unknown: 3.2576 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 17s 832ms/step - loss: 4818905.5000 - mse_known: 1.7120 - mse_unknown: 3.2617 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 19s 959ms/step - loss: 4770989.0000 - mse_known: 1.7037 - mse_unknown: 3.2654 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [54.03577, 102.034676]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 124\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 25s 1s/step - loss: 707190.1250 - mse_known: 0.0404 - mse_unknown: 0.3501 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 16s 790ms/step - loss: 705201.3750 - mse_known: 0.0402 - mse_unknown: 0.3505 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 16s 774ms/step - loss: 703208.9375 - mse_known: 0.0400 - mse_unknown: 0.3508 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 18s 919ms/step - loss: 701361.8750 - mse_known: 0.0398 - mse_unknown: 0.3511 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 18s 894ms/step - loss: 699835.3125 - mse_known: 0.0397 - mse_unknown: 0.3514 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [57.850616, 118.12245]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 126\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 18s 884ms/step - loss: 877650.5000 - mse_known: 0.0888 - mse_unknown: 0.5264 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 19s 950ms/step - loss: 877520.8750 - mse_known: 0.0888 - mse_unknown: 0.5263 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 17s 861ms/step - loss: 877390.0000 - mse_known: 0.0888 - mse_unknown: 0.5263 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 18s 909ms/step - loss: 877270.8750 - mse_known: 0.0888 - mse_unknown: 0.5263 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 17s 852ms/step - loss: 877165.8125 - mse_known: 0.0889 - mse_unknown: 0.5263 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [59.41733, 120.435844]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 127\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 18s 894ms/step - loss: 3015703.5000 - mse_known: 0.5509 - mse_unknown: 1.5103 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 18s 929ms/step - loss: 3012884.5000 - mse_known: 0.5507 - mse_unknown: 1.5107 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 751ms/step - loss: 3010038.2500 - mse_known: 0.5506 - mse_unknown: 1.5109 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 19s 953ms/step - loss: 3007369.5000 - mse_known: 0.5504 - mse_unknown: 1.5112 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 18s 881ms/step - loss: 3005164.2500 - mse_known: 0.5502 - mse_unknown: 1.5115 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [56.8565, 121.3467]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 129\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 17s 814ms/step - loss: 486094.8438 - mse_known: 1.0789 - mse_unknown: 2.2302 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 16s 785ms/step - loss: 485895.0625 - mse_known: 1.0783 - mse_unknown: 2.2302 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 768ms/step - loss: 485699.3438 - mse_known: 1.0776 - mse_unknown: 2.2303 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 17s 872ms/step - loss: 485521.6875 - mse_known: 1.0771 - mse_unknown: 2.2303 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 15s 740ms/step - loss: 485374.1875 - mse_known: 1.0766 - mse_unknown: 2.2303 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [59.34573, 119.38869]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 133\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 15s 752ms/step - loss: 862250.3750 - mse_known: 0.0903 - mse_unknown: 0.4565 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 17s 763ms/step - loss: 862134.8125 - mse_known: 0.0902 - mse_unknown: 0.4568 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 764ms/step - loss: 862022.6250 - mse_known: 0.0902 - mse_unknown: 0.4570 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 15s 758ms/step - loss: 861915.3750 - mse_known: 0.0901 - mse_unknown: 0.4572 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 14s 724ms/step - loss: 861819.1250 - mse_known: 0.0900 - mse_unknown: 0.4574 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [59.446247, 119.609856]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 134\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 15s 746ms/step - loss: 1295010.7500 - mse_known: 0.0684 - mse_unknown: 0.4601 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 16s 790ms/step - loss: 1291736.5000 - mse_known: 0.0678 - mse_unknown: 0.4604 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 17s 857ms/step - loss: 1288408.3750 - mse_known: 0.0673 - mse_unknown: 0.4607 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 15s 757ms/step - loss: 1285268.1250 - mse_known: 0.0668 - mse_unknown: 0.4609 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 16s 805ms/step - loss: 1282619.2500 - mse_known: 0.0664 - mse_unknown: 0.4611 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [57.640263, 117.156334]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 135\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 15s 774ms/step - loss: 728825.1875 - mse_known: 0.0382 - mse_unknown: 0.3352 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 17s 847ms/step - loss: 727392.1875 - mse_known: 0.0380 - mse_unknown: 0.3354 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 17s 877ms/step - loss: 725922.3750 - mse_known: 0.0378 - mse_unknown: 0.3357 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 21s 1s/step - loss: 724572.3750 - mse_known: 0.0376 - mse_unknown: 0.3360 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 18s 911ms/step - loss: 723443.8125 - mse_known: 0.0375 - mse_unknown: 0.3362 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [58.68508, 117.955154]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 142\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 18s 914ms/step - loss: 1000714.6250 - mse_known: 0.0685 - mse_unknown: 0.3936 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 20s 1s/step - loss: 1000276.0000 - mse_known: 0.0684 - mse_unknown: 0.3938 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 19s 929ms/step - loss: 999836.1250 - mse_known: 0.0682 - mse_unknown: 0.3939 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 19s 969ms/step - loss: 999413.1875 - mse_known: 0.0681 - mse_unknown: 0.3940 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 15s 774ms/step - loss: 999066.1875 - mse_known: 0.0680 - mse_unknown: 0.3941 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [58.878155, 119.26126]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 143\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 19s 919ms/step - loss: 1103284.1250 - mse_known: 0.0675 - mse_unknown: 0.4466 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 16s 808ms/step - loss: 1102182.8750 - mse_known: 0.0672 - mse_unknown: 0.4468 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 19s 929ms/step - loss: 1101063.2500 - mse_known: 0.0669 - mse_unknown: 0.4470 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 16s 794ms/step - loss: 1099997.6250 - mse_known: 0.0667 - mse_unknown: 0.4471 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 20s 985ms/step - loss: 1099103.7500 - mse_known: 0.0664 - mse_unknown: 0.4473 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [58.848053, 118.185455]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 146\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 20s 1s/step - loss: 767285.3750 - mse_known: 0.0710 - mse_unknown: 0.3610 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 20s 992ms/step - loss: 766615.2500 - mse_known: 0.0708 - mse_unknown: 0.3612 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 15s 755ms/step - loss: 765883.0000 - mse_known: 0.0707 - mse_unknown: 0.3615 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 14s 702ms/step - loss: 765270.3750 - mse_known: 0.0706 - mse_unknown: 0.3617 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 14s 716ms/step - loss: 764736.6250 - mse_known: 0.0705 - mse_unknown: 0.3619 - lr: 6.9444e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filter Coefficients: [58.587578, 119.189]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n",
      "Datapoint: 151\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 15s 733ms/step - loss: 1206623.7500 - mse_known: 0.2902 - mse_unknown: 0.7398 - lr: 1.0000e-05\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 14s 715ms/step - loss: 1205140.7500 - mse_known: 0.2907 - mse_unknown: 0.7396 - lr: 1.0000e-05\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 18s 921ms/step - loss: 1203727.2500 - mse_known: 0.2912 - mse_unknown: 0.7395 - lr: 1.0000e-05\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 18s 889ms/step - loss: 1202483.5000 - mse_known: 0.2916 - mse_unknown: 0.7394 - lr: 8.3333e-06\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 17s 867ms/step - loss: 1201485.7500 - mse_known: 0.2919 - mse_unknown: 0.7393 - lr: 6.9444e-06\n",
      "\n",
      "Filter Coefficients: [59.702103, 122.374374]\n",
      "\n",
      "Learned Graph Shape:\n",
      "(32, 32)\n"
     ]
    }
   ],
   "source": [
    "# re_run_idx = [12, 37, 81, 97, 118, 124, 126, 127, 129, 133, 134, 135, 142, 143, 146, 151]\n",
    "\n",
    "# all_vars = []\n",
    "# for dat in re_run_idx:\n",
    "#     dat = dat - 1\n",
    "#     print(f\"Datapoint: {dat + 1}\")\n",
    "#     model_pred = []\n",
    "#     loss_model = []\n",
    "#     learned_graph = []\n",
    "\n",
    "#     X_train = X_train_data[:, :, dat]\n",
    "#     Lap_init = initial_graph(X_train, N, 6)\n",
    "    \n",
    "#     all_psi = []\n",
    "\n",
    "#     M = int(sensing_ratio * T)\n",
    "#     X_train_missing = np.zeros([no_of_psi, N, T])\n",
    "#     X_train_concatenated = np.zeros([no_of_psi, N, 2 * T])\n",
    "\n",
    "#     for i in range(no_of_psi):\n",
    "#         psi_k = np.array([0] * (N * M) + [1] * (N * (T - M)))\n",
    "#         np.random.shuffle(psi_k)\n",
    "#         psi_k = psi_k.reshape([N, T])\n",
    "#         all_psi.append(psi_k)\n",
    "#         X_train_missing[i, : , : ] = X_train * psi_k\n",
    "#         X_train_concatenated[i, :, :] = np.concatenate((X_train_missing[i], psi_k), axis = 1)\n",
    "    \n",
    "#     coeffs = coeffs_list[dat, :]\n",
    "\n",
    "#     sgd_optimizer = tf.keras.optimizers.SGD(learning_rate = 0.00001, momentum = 0.0)\n",
    "    \n",
    "#     our_model = ProposedModel(Lap_init, reg_init, [loop_a, loop_b, loop_c], \n",
    "#                               d, p_init, alpha_init, beta_init, threshold_init, name = 'our')\n",
    "#     our_model.compile(optimizer = sgd_optimizer, loss = costfunc,\n",
    "#                       metrics = [mse_known, mse_unknown], run_eagerly=True)\n",
    "    \n",
    "#     our_model.fit(X_train_concatenated, X_train_missing, epochs = 5, \n",
    "#                   callbacks = [lr_scheduler], batch_size = 1)\n",
    "    \n",
    "#     var = our_model.variables\n",
    "#     var_lst = []\n",
    "#     for v in var:\n",
    "#         var_lst.append(v.numpy()[0, 0])\n",
    "#     print(f\"\\nFilter Coefficients:\", var_lst)\n",
    "#     all_vars.append(var_lst)\n",
    "\n",
    "#     graph = learned_graph[-1]\n",
    "#     print(f\"\\nLearned Graph Shape:\\n{graph.shape}\")\n",
    "\n",
    "#     np.save(f\"./Outputs/Graphs/OpenNEURO/{label_print(dat, 3)}_{y_train_data[dat]}.npy\", graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
